# The name of the Storm DRPC function
bullet.topology.function: "tracer"

# The name of the Storm topology
bullet.topology.name: "bullet-topology"

# The number of topology workers to use. Applicable when the scheduler is not ras.
bullet.topology.workers: 92

# Enable Storm debug logging
bullet.topology.debug: false

# The scheduling strategy to use. only "ras" currently supported.
bullet.topology.scheduler: "ras"

# Enable metrics collection for the topology. The registered metrics classes under bullet.topology.metrics.classes are loaded.
# This enables the default storm built-in metrics. See: http://storm.apache.org/releases/1.0.1/Metrics.html
# Depending on your cluster configuration, things such as on/off heap usage, cpu usage, garbage collection time/types
# send/receive queue sizes and overflows, execute/ack/fail counts and other such system metrics can be obtained.
bullet.topology.metrics.enable: false

# Enable the collection of the Bullet built-in metrics. These will be sent to your metrics classes. This is in addition
# to the storm built-in metrics. Currently supported are:
# Counting metrics: These behave like org.apache.storm.metric.api.CountMetric but do not reset their value when getValueAndReset()
# is called. This custom metric is defined in com.yahoo.bullet.storm.AbsoluteCountMetric.
#   1. bullet_active_queries : A metric that counts the number of queries active per JoinBolt. Note that this metric will
#                            increase and decrease as queries come and go. Make sure to configure the emit period accordingly
#                            otherwise short running queries may not be even counted.
#   2. bullet_created_queries : A metric that counts the number of queries created per JoinBolt.
#   3. bullet_improper_queries : A metric that counts the number of semantically incorrect queries created per JoinBolt.
bullet.topology.metrics.built.in.enable: false
# The time period mapping from metric name to seconds for emitting the metric. This is passed to Storm when the metrics are registered.
# A default value for all metrics can also be set by using the "default" key.
bullet.topology.metrics.built.in.emit.interval.mapping:
   bullet_active_queries: 10
   default: 10

# You can list full package prefixed class path to your custom implementation. The class must extend
# org.apache.storm.metric.api.IMetricsConsumer. Additionally, it must provide a static method with signature
# public static void register(Config, BulletConfig), where  Config is of type org.apache.storm.Config and
# BulletConfig is of type com.yahoo.bullet.BulletConfig.
# Your register method is responsible for calling registerMetricsConsumer to add itself to the provided
# Config. You can add settings such as your argument for the prepare method call, or your parallelism for the
# IMetricsConsumer in this register method. Any custom metrics that you have should also be added to Config by
# adding (not replacing) to the Config.TOPOLOGY_WORKER_METRICS map. For an example, see the implementation of
# of the com.yahoo.bullet.storm.SigarLoggingMetricsConsumer.
bullet.topology.metrics.classes:
  # This uses the LoggingMetricsConsumer and collects a CPU metric using org.apache.storm.metrics.sigar.CPUMetric. It
  # runs with a parallelism of 1. The CPU metric is a worker level metric.
  - "com.yahoo.bullet.storm.SigarLoggingMetricsConsumer"

# The following CPU loads and memory on and off heap control their respective component's CPU
# and memory configuration. These settings are only used when scheduler is "ras"
# The parallelism setting controls the number of executors used for each component.
bullet.topology.drpc.spout.cpu.load: 20.0
bullet.topology.drpc.spout.memory.on.heap.load: 256.0
bullet.topology.drpc.spout.memory.off.heap.load: 160.0
bullet.topology.drpc.spout.parallelism: 20
bullet.topology.prepare.bolt.cpu.load: 20.0
bullet.topology.prepare.bolt.memory.on.heap.load: 128.0
bullet.topology.prepare.bolt.memory.off.heap.load: 160.0
bullet.topology.prepare.bolt.parallelism: 5
bullet.topology.return.bolt.cpu.load: 20.0
bullet.topology.return.bolt.memory.on.heap.load: 128.0
bullet.topology.return.bolt.memory.off.heap.load: 160.0
bullet.topology.return.bolt.parallelism: 10
bullet.topology.filter.bolt.cpu.load: 100.0
bullet.topology.filter.bolt.memory.on.heap.load: 256.0
bullet.topology.filter.bolt.memory.off.heap.load: 160.0
bullet.topology.filter.bolt.parallelism: 35
bullet.topology.join.bolt.cpu.load: 100.0
bullet.topology.join.bolt.memory.on.heap.load: 512.0
bullet.topology.join.bolt.memory.off.heap.load: 160.0
bullet.topology.join.bolt.parallelism: 20

# Bullet uses tick tuples underneath the hood as a "clock" mechanism to do metadata and query updates (checking if queries
# have expired) etc. This setting controls the how frequently a tick happens - number of seconds between ticks.
bullet.topology.tick.interval.secs: 5

# This is the number of ticks for which an error caused by receiving a bad query will be buffered if the
# return information has not been received, will be buffered before being thrown away
bullet.topology.join.bolt.error.tick.timeout: 3

# This is the number of ticks for which a query will be buffered past its expiry in order to wait for
# aggregations to trickle in from the Filter Bolts.
bullet.topology.join.bolt.query.tick.timeout: 3

# The default duration in milliseconds for a query if one has not been specified.
bullet.query.default.duration: 30000

# The maximum duration in milliseconds allowed for a query. Anything greater will be clamped to this value.
bullet.query.max.duration: 120000

# The default number of records that can be aggregated for a query if one has not been specified.
bullet.query.aggregation.default.size: 1

# The maximum number of records that will be aggregated per query. Anything greater will be clamped to this value.
bullet.query.aggregation.max.size: 512

# This is the separator that is used when a set of fields has to be considered as a single String.
# This is relevant when hashing a set of fields (for example, in a GROUP operation) for uniqueness purposes, such
# as when inserting into a Sketch. Without this, for example, if you were considering two fields together as a
# group, with values ab and cd, simply concatenating them would produce abcd. This is ambiguous if you with another
# record that had values a and bcd for those two fields. Using this separator distinguishes them for this purpose.
# If the default separator occurs in your fields, you should change it something else.
bullet.query.aggregation.composite.field.separator: "|"

# The maximum number of records that will be collected in the Filter Bolt till it is emitted - i.e. a micro-batch.
# Leaving this at 1 emits your raw aggregation records as soon as they are received in the Filter Bolt. This makes
# your raw aggregation query run snappier if the total number of matched records across the Filter Bolts exceeds
# the number of records your query is looking for but individually each Filter Bolt does not find enough records to
# satisfy the query. Since the records are emitted immediately, the Join Bolt will terminate your query as soon
# as the total records are received instead of waiting for the micro-batch size to be reached.
# If you set this too high (for example, higher than the query size), you will wait the entire duration of the query,
# and the number of ticks specified in topology.join.bolt.query.tick.timeout.
bullet.query.aggregation.raw.micro.batch.size: 1

# The maximum number of records that will be collected for Raw aggregations. This number should be <= to the
# query.aggregation.max.size value. If it is greater, the aggregation size will be used.
bullet.query.aggregation.raw.max.size: 30

# The maximum number of entries stored by a Sketch created for doing COUNT DISTINCTS. Decreasing this number
# (rounded to powers of 2) can decrease the accuracy for high cardinality dimensions while decreasing the total
# memory used by the Sketch. The errors for a Theta Sketch is fixed at a maximum when this number is chosen - in other
# words, the error will be no worse than this maximum regardless of how many unique entries are inserted into the
# Sketch. Refer to: https://datasketches.github.io/docs/Theta/ThetaErrorTable.html
bullet.query.aggregation.count.distinct.sketch.entries: 16384

# Controls how much sampling is done by the Sketch for COUNT DISTINCTS. A value of 1.0 means no sampling is done.
# A value of 0.5 means, the Sketch will throw out half the data coming into the Sketch.
# You can leave this at 1 since it really only affects it when we start supporting COUNT DISTINCTS as GROUP operations.
# https://datasketches.github.io/docs/Theta/ThetaPSampling.html
bullet.query.aggregation.count.distinct.sketch.sampling: 1.0

# This can either be QuickSelect or Alpha (CaSe SeNsiTive). You can leave this at the default.
# Alpha Sketches are 30% more accurate if their estimates are queried directly but since we union them, their accuracy
# reverts back to the QuickSelect accuracy. Alpha Sketches are also faster when updating.
# https://datasketches.github.io/docs/Theta/ThetaUpdateSpeed.html
bullet.query.aggregation.count.distinct.sketch.family: "Alpha"

# A Sketch does not start the maximum size specified tbe sketch.entries setting. It grows toward it and can be at most
# 2 x the size at the maximum. This factor controls by how much the size grows when the threshold is reached. Valid
# values are 1 (no resize start at maximum), 2 (double), 4 (quadruple) and 8 (octuple). Any other value defaults to 8.
# https://datasketches.github.io/docs/Theta/ThetaUpdateSpeed.html
bullet.query.aggregation.count.distinct.sketch.resize.factor: 8

# The maximum number of entries stored by a Sketch created for doing GROUP BY. Sketches are used to do a uniform
# sample across your unique groups. So, this value should be set to a power of 2 approximately equal to your value for
# bullet.query.aggregation.max.size. Anything greater will still work but the aggregation max size will limit your result
# anyway, so it's just a waste of resources to do so. If you have a count or sum as a metric for the group, summing them
# across the groups and dividing by your Sketch Theta (in the metadata), gives you an approximate estimate of the real
# sum/count across all your actual groups. The error is defined by the QuickSelect Sketch error. Refer to:
# https://datasketches.github.io/docs/Theta/ThetaErrorTable.html
bullet.query.aggregation.group.sketch.entries: 512

# Controls how much sampling is done by the Sketch for GROUP BY. A value of 1.0 means no sampling is done.
# A value of 0.5 means, the Sketch will throw out half the data coming into the Sketch.
# You can leave this at 1 since it really only affects it when we start supporting COUNT DISTINCTS as GROUP operations.
# https://datasketches.github.io/docs/Theta/ThetaPSampling.html
bullet.query.aggregation.group.sketch.sampling: 1.0

# A Sketch does not start the maximum size specified tbe sketch.entries setting. It grows toward it and can be at most
# 2 x the size at the maximum. This factor controls by how much the size grows when the threshold is reached. Valid
# values are 1 (no resize start at maximum), 2 (double), 4 (quadruple) and 8 (octuple). Any other value defaults to 8.
# https://datasketches.github.io/docs/Theta/ThetaUpdateSpeed.html
bullet.query.aggregation.group.sketch.resize.factor: 8

# The maximum number of entries stored by a Quantile Sketch created for doing DISTRIBUTIONS. Decreasing this number
# (rounded to powers of 2) can increase the normalized error while decreasing the total memory used by the Sketch.
# The normalized error for a Quantile Sketch is fixed at a maximum when this number is chosen - in other
# words, the error will be no worse than this maximum regardless of how many entries are inserted into the
# Sketch. Refer to: https://datasketches.github.io/docs/Quantiles/QuantilesAccuracy.html
bullet.query.aggregation.distribution.sketch.entries: 1024

# The maximum number of points that can be provided or generated for the DISTRIBUTION aggregation. Any more will be
# clamped to this value. These points are used to pick at the quantiles, or the PMF or the CDF.
bullet.query.aggregation.distribution.max.points: 100

# Enable logging meta information in the results. Configured metadata will be add to the meta section of the
# results: {"meta": {}, "records": []}
bullet.result.metadata.enable: true

# Each entry in this list indicates which metadata to collect (the name) and what key to add it as (the key) to the meta
# Query Identifier adds the original DRPC ID that was generated for the query.
# Query Body adds the received query definition. This is useful for diagnosing syntax exceptions when errors are received.
# Query Creation Time adds the timestamp in milliseconds when the query was received by the Join Bolt
# Query Termination Time adds the timestamp in milliseconds when the result was emitted by the Join Bolt

# Sketch Metadata adds additional nested metadata about sketches if set. These are listed below.
# Estimated Result adds a boolean denoting whether the result was estimated. (COUNT DISTINCT, GROUP, DISTRIBUTION)
# Standard Deviations adds an object inside the Aggregation Metadata object where the keys are the standard deviations
#                     and the values are objects containing upper and lower bounds (COUNT DISTINCT, GROUP)
# Family adds the family of Sketches uses to produce the result, if one was used. (COUNT DISTINCT, GROUP, DISTRIBUTION)
# Size adds the size of final Sketch used to produced the result, if one was used. (COUNT DISTINCT, DISTRIBUTION)
# Theta adds the theta value of the Sketch for Theta and Tuple Sketches, if one was used. (COUNT DISTINCT, GROUP)
# Uniques Estimate adds the approximate unique values seen for Tuple Sketches. (GROUP)
# Minimum Value adds the smallest value seen for Quantile Sketches. (DISTRIBUTION)
# Maximum Value adds the largest value seen for Quantile Sketches. (DISTRIBUTION)
# Items Seen adds the number of items inserted into Quantile Sketches. (DISTRIBUTION)
# Normalized Rank Error adds the error of the Quantile Sketch as a double between 0 and 1.0. If this was 0.002, for e.g, then
#                       the error bound on a quantile estimate like median would be : 49.8% >= median <= 50.2%. (DISTRIBUTION)
bullet.result.metadata.metrics:
    - name: "Query Identifier"
      key: "query_id"
    - name: "Query Body"
      key: "query"
    - name: "Query Creation Time"
      key: "query_receive_time"
    - name: "Query Termination Time"
      key: "query_finish_time"
    - name: "Sketch Metadata"
      key: "sketches"
    - name: "Estimated Result"
      key: "was_estimated"
    - name: "Standard Deviations"
      key: "standard_deviations"
    - name: "Family"
      key: "family"
    - name: "Size"
      key: "size"
    - name: "Theta"
      key: "theta"
    - name: "Uniques Estimate"
      key: "uniques_estimate"
    - name: "Minimum Value"
      key: "minimum_value"
    - name: "Maximum Value"
      key: "maximum_value"
    - name: "Items Seen"
      key: "items_seen"
    - name: "Normalized Rank Error"
      key: "normalized_rank_error"

# Enables whether each record should have a new key added to it denoting when the Filter Bolt saw it
bullet.record.inject.timestamp.enable: true
# This is the key that is used to add the timestamp in milliseconds to the record, if record.inject.timestamp.enable is true
bullet.record.inject.timestamp.key: "bullet_receive_timestamp"
